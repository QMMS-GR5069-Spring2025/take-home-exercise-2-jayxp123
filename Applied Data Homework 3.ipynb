{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "973bc961-1ad7-457d-8eee-d8a4734194e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Author: Jay Jun\n",
    "# Date: March 22, 2025\n",
    "# Project: Applied Data Homework #3\n",
    "# Purpose: Answer question using F1 data on the AWS S3 utilizing Databricks using either Pandas, R , or PySpark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb2860d-92ca-4871-8d29-8a561783f477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, round, avg, concat_ws, rank, row_number, round, col, when, udf, year, datediff, count, lit, when\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F1 Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the pit stops data\n",
    "pit_stops_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/pit_stops.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the drivers data for reference\n",
    "drivers_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the results data for reference \n",
    "results_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/results.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the races data for reference\n",
    "races_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/races.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the status data for reference \n",
    "status_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/status.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the qualifying data for reference \n",
    "qualifying_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/qualifying.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b25c4a-db20-4ec5-8185-24690b41f39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The first question asks \"What was the average time each driver spent at the pit stop for each race?\"\n",
    "\n",
    "#Joining pit stops and drivers data (specfically selecting columns that are needed)\n",
    "\n",
    "drivers_df = drivers_df.select(\"driverId\", \"forename\", \"surname\")\n",
    "pit_stops_with_names = pit_stops_df.join(drivers_df, \"driverId\")\n",
    "\n",
    "# Select only the necessary columns from drivers\n",
    "drivers_df = drivers_df.select(\"driverId\", \"forename\", \"surname\")\n",
    "\n",
    "# Calculate average pit stop duration for each driver in each race\n",
    "avg_pit_stop_times = pit_stops_df.groupBy(\"raceId\", \"driverId\") \\\n",
    "    .agg(round(avg(\"duration\"), 3).alias(\"avg_duration\"))\n",
    "\n",
    "# Join with drivers_df to add driver names\n",
    "result = avg_pit_stop_times.join(drivers_df, \"driverId\") \\\n",
    "    .withColumn(\"driver_name\", concat_ws(\" \", trim(col(\"forename\")), trim(col(\"surname\")))) \\\n",
    "    .select(\"raceId\", \"driver_name\", \"avg_duration\") \\\n",
    "    .orderBy(\"raceId\", \"avg_duration\")\n",
    "\n",
    "# Show the results\n",
    "result.show(100, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae793042-d846-4c15-83d1-8ea34b879fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The second question asks us to \"rank the average time spent at the pit stop in order of who won each race\"\n",
    "\n",
    "# Join results with pit stops\n",
    "joined_df = results_df.join(pit_stops_df, on=[\"raceId\", \"driverId\"], how=\"inner\")\n",
    "\n",
    "# Compute average pit stop time per driver per race and position\n",
    "avg_pit_df = joined_df.groupBy(\"raceId\", \"driverId\", \"positionOrder\").agg(\n",
    "    avg(pit_stops_df.milliseconds).alias(\"avg_pit_stop_time\")\n",
    ")\n",
    "\n",
    "# Define window by raceId, ordered by finishing position\n",
    "window_spec = Window.partitionBy(\"raceId\").orderBy(\"positionOrder\")\n",
    "\n",
    "# Rank drivers by their finish within each race\n",
    "ranked_df = avg_pit_df.withColumn(\"avg_pit_stop_time\", round(avg_pit_df[\"avg_pit_stop_time\"], 2)) \\\n",
    "                      .withColumn(\"finishing_rank\", row_number().over(window_spec))\n",
    "# Sort and display the final result\n",
    "final_df = ranked_df.orderBy(\"raceId\", \"finishing_rank\")\n",
    "final_df.show(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ee4c33-d926-491f-af17-ced0869d2ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The third question asks insert the missing code (e.g: ALO for Alonso) for drivers based on the 'drivers' dataset\n",
    "\n",
    "# Define a user defined function (UDF) to generate the missing codes\n",
    "def generate_code(surname):\n",
    "    return surname[:3].upper()\n",
    "\n",
    "generate_code_udf = udf(generate_code, StringType())\n",
    "\n",
    "# Apply the (UDF) to fill in the missing codes\n",
    "drivers_df_filled = drivers_df.withColumn(\n",
    "    \"code\",\n",
    "    when(col(\"code\") == \"\\\\N\", generate_code_udf(col(\"surname\"))).otherwise(col(\"code\"))\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "drivers_df_filled.select(\"driverId\", \"forename\", \"surname\", \"code\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79599d41-097b-45b1-84d9-134c2482e8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#The fourth question \"Who is the youngest and oldest driver for each race? Create a new column called “Age”\n",
    "\n",
    "# The fourth question \"Who is the youngest and oldest driver for each race? Create a new column called “Age”\"\n",
    "\n",
    "# Join drivers and results dataframes\n",
    "joined_df = drivers_df.join(results_df, \"driverId\").join(races_df, \"raceId\")\n",
    "\n",
    "# Calculate age at the time of the race\n",
    "age_df = joined_df.withColumn(\n",
    "    \"Age\",\n",
    "    round(\n",
    "        when(\n",
    "            (year(col(\"date\")) > year(col(\"dob\"))) |\n",
    "            ((year(col(\"date\")) == year(col(\"dob\"))) & (col(\"date\") >= col(\"dob\"))),\n",
    "            datediff(col(\"date\"), col(\"dob\")) / 365\n",
    "        ).otherwise(datediff(col(\"date\"), col(\"dob\")) / 365 - 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Find youngest and oldest drivers for each race\n",
    "window_spec = Window.partitionBy(\"raceId\")\n",
    "result_df = age_df.withColumn(\"Youngest_Driver\", F.min(\"Age\").over(window_spec)) \\\n",
    "                  .withColumn(\"Oldest_Driver\", F.max(\"Age\").over(window_spec)) \\\n",
    "                  .select(\"raceId\", \"date\", \"Youngest_Driver\", \"Oldest_Driver\") \\\n",
    "                  .distinct() \\\n",
    "                  .orderBy(\"raceId\")\n",
    "\n",
    "# Show the results\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13506f3-041d-4fbb-b0c2-61200d1a8c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Queston 5 asks \"For a given race, which driver has the most wins and losses?\"\n",
    "\n",
    "# Identify all statusIds that represent a \"Did Not Finish\" (DNF)\n",
    "# Usually any description that is NOT \"Finished\" is a DNF\n",
    "dnf_status_ids = status_df.filter(col(\"status\") != \"Finished\").select(\"statusId\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filter to races before the given race\n",
    "target_race_id = 843\n",
    "previous_races_df = results_df.filter(col(\"raceId\") < target_race_id)\n",
    "\n",
    "# Create labeled columns\n",
    "labeled_df = previous_races_df.withColumn(\n",
    "    \"win\", when(col(\"positionOrder\") == 1, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"not_completed\", when(col(\"statusId\").isin(dnf_status_ids), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"completed_not_won\", when(\n",
    "        (col(\"positionOrder\") > 1) & (~col(\"statusId\").isin(dnf_status_ids)), 1\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"total_participated\", lit(1)\n",
    ")\n",
    "\n",
    "# Aggregate\n",
    "summary_df = labeled_df.groupBy(\"driverId\").agg(\n",
    "    count(when(col(\"win\") == 1, True)).alias(\"wins\"),\n",
    "    count(when(col(\"completed_not_won\") == 1, True)).alias(\"completed_not_won\"),\n",
    "    count(when(col(\"not_completed\") == 1, True)).alias(\"not_completed\"),\n",
    "    count(col(\"total_participated\")).alias(\"total_races\")\n",
    ")\n",
    "\n",
    "# Join with driver names\n",
    "final_df = summary_df.join(drivers_df.select(\"driverId\", \"surname\"), on=\"driverId\", how=\"left\")\n",
    "\n",
    "# Display results\n",
    "final_df.select(\"surname\", \"wins\", \"completed_not_won\", \"not_completed\", \"total_races\") \\\n",
    "    .orderBy(\"wins\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585348ef-007a-4edb-ba22-801fd829a9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A queston I will be answeing will be \"Which driver improves the most positions, on average, from their qualifying position to thier race finishing position?\"\n",
    "\n",
    "# Join on raceId and driverId to get both qualifying and result info\n",
    "joined_df = qualifying_df.join(\n",
    "    results_df.select(\"raceId\", \"driverId\", \"positionOrder\"),\n",
    "    on=[\"raceId\", \"driverId\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Calculate position change (positive = improved positions)\n",
    "position_diff_df = joined_df.withColumn(\n",
    "    \"position_gain\", col(\"position\") - col(\"positionOrder\")\n",
    ")\n",
    "\n",
    "# Group by driver and calculate average gain, rounding to the nearest whole number\n",
    "avg_gain_df = position_diff_df.groupBy(\"driverId\").agg(\n",
    "    round(avg(\"position_gain\")).alias(\"avg_position_gain\")\n",
    ")\n",
    "\n",
    "# Join with driver names\n",
    "final_gain_df = avg_gain_df.join(\n",
    "    drivers_df.select(\"driverId\", \"surname\"),\n",
    "    on=\"driverId\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Order by most average positions gained\n",
    "display(\n",
    "    final_gain_df.select(\"surname\", \"avg_position_gain\")\n",
    "    .orderBy(\"avg_position_gain\", ascending=False)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Applied Data Homework 3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
