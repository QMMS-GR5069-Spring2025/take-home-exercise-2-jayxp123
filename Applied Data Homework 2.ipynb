{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "973bc961-1ad7-457d-8eee-d8a4734194e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Author: Jay Jun\n",
    "# Date: March 22, 2025\n",
    "# Project: Applied Data Homework #2\n",
    "# Purpose: Answer question using F1 data on the AWS S3 utilizing Databricks using either Pandas, R , or PySpark \n",
    "# Inputs: raw/pit_stops.csv, raw/drivers.csv, raw/results.csv, raw/races.csv, raw/status.csv, raw/qualifying.csv\n",
    "# Outputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506d2d91-d5bc-42b6-b067-69448e4f85aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new folder in my AWS bucket to put all my results in \n",
    "dbutils.fs.mkdirs(\"s3://jwj2123-gr5069/processed/Assignment #2/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cb2860d-92ca-4871-8d29-8a561783f477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, round, avg, concat_ws, rank, row_number, round, when, udf, year, datediff, count, lit, when, to_date, month, dayofmonth\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"F1 Data Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the pit stops data\n",
    "pit_stops_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/pit_stops.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the drivers data for reference\n",
    "drivers_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/drivers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the results data for reference \n",
    "results_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/results.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the races data for reference\n",
    "races_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/races.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the status data for reference \n",
    "status_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/status.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load the qualifying data for reference \n",
    "qualifying_df = spark.read.csv(\"s3://columbia-gr5069-main/raw/qualifying.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14adfa12-f9d9-497d-865f-12ce90b92e30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Q1:[10 pts] What was the average time each driver spent at the pit stop for each race?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5b25c4a-db20-4ec5-8185-24690b41f39e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Joining pit stops and drivers data (specfically selecting columns that are needed)\n",
    "\n",
    "drivers_df = drivers_df.select(\"driverId\", \"forename\", \"surname\")\n",
    "pit_stops_with_names = pit_stops_df.join(drivers_df, \"driverId\")\n",
    "\n",
    "# Select only the necessary columns from drivers\n",
    "drivers_df = drivers_df.select(\"driverId\", \"forename\", \"surname\")\n",
    "\n",
    "# Calculate average pit stop duration for each driver in each race\n",
    "avg_pit_stop_times = pit_stops_df.groupBy(\"raceId\", \"driverId\") \\\n",
    "    .agg(round(avg(\"duration\"), 3).alias(\"avg_duration\"))\n",
    "\n",
    "# Join with drivers_df to add driver names\n",
    "result = avg_pit_stop_times.join(drivers_df, \"driverId\") \\\n",
    "    .withColumn(\"driver_name\", concat_ws(\" \", trim(col(\"forename\")), trim(col(\"surname\")))) \\\n",
    "    .select(\"raceId\", \"driver_name\", \"avg_duration\") \\\n",
    "    .orderBy(\"raceId\", \"avg_duration\")\n",
    "\n",
    "# Show the results\n",
    "result.show(100, False)\n",
    "\n",
    "# Outputting to AWS\n",
    "result.coalesce(1).write.mode('overwrite').option(\"header\", True)\\\n",
    "    .csv(\"s3://jwj2123-gr5069/processed/Assignment #2/average_time/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce121b1b-5bda-489b-a75e-42d61bbe5d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Q2:[20 pts] Rank the average time spent at the pit stop in order of who won each race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1840b0d-7840-4173-b8a2-95fa731988dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the average pit stop time per driver and race (rounded to 2 decimal places) with average pit time calcualted in milliseconds\n",
    "avg_pit_stop_time_df = pit_stops_df.groupBy(\"raceId\", \"driverId\") \\\n",
    "    .agg(round(avg(\"milliseconds\"), 2).alias(\"avg_pit_stop_time\"))\n",
    "\n",
    "# Join with the results dataset to bring in the finishing order (using 'positionOrder')\n",
    "# We use 'positionOrder' instead of 'position' because 'positionOrder' actually shows the finishing position instead of a null value. \n",
    "# It looks like 'positionOrder' actually puts a position in the race, even if they did not finish the race\n",
    "avg_pit_stop_with_results = avg_pit_stop_time_df.join(\n",
    "    results_df.select(\"raceId\", \"driverId\", \"positionOrder\"),\n",
    "    on=[\"raceId\", \"driverId\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Adding some driver names\n",
    "avg_pit_stop_with_results = avg_pit_stop_with_results.join(\n",
    "    drivers_df, on=\"driverId\", how=\"left\"\n",
    ").withColumn(\"driver_name\", concat_ws(\" \", \"forename\", \"surname\"))\n",
    "\n",
    "# Adding the race names\n",
    "avg_pit_stop_with_results = avg_pit_stop_with_results.join(\n",
    "    races_df.select(\"raceId\", \"name\"),\n",
    "    on=\"raceId\",\n",
    "    how=\"left\"\n",
    ").withColumnRenamed(\"name\", \"race_name\")\n",
    "\n",
    "# Define a window partitioned by raceId, ordering by positionOrder.\n",
    "# Using asc_nulls_last() ensures that if any non-finishers (with null in positionOrder) exist, they are placed at the end. Putting this in just in case there are null values in thedata\n",
    "window_spec = Window.partitionBy(\"raceId\").orderBy(col(\"positionOrder\").asc_nulls_last())\n",
    "\n",
    "# Create a new column with the finishing rank based on the race results\n",
    "ranked_result = avg_pit_stop_with_results.withColumn(\"finishing_rank\", rank().over(window_spec))\n",
    "\n",
    "# Order the final DataFrame by raceId, finishing_rank, and driverId for consistency\n",
    "final_df = ranked_result.orderBy(\"raceId\", \"finishing_rank\", \"driverId\")\n",
    "\n",
    "# Displaying the results\n",
    "final_df.select(\"raceId\", \"race_name\", \"driverId\", \"driver_name\", \n",
    "                \"avg_pit_stop_time\", \"positionOrder\", \"finishing_rank\").show(20)\n",
    "\n",
    "# Outputting final ranked results to AWS as CSV\n",
    "final_df.coalesce(1).write.mode('overwrite').option(\"header\", True)\\\n",
    "    .csv(\"s3://jwj2123-gr5069/processed/Assignment #2/rank_average_time/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2e671d-3fe8-44c4-b8e7-5d7818522e6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3.[20 pts] Insert the missing code (e.g: ALO for Alonso) for drivers based on the 'drivers' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ee4c33-d926-491f-af17-ced0869d2ae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  To fill in the rest of the missing code, I coded for the first three letters of their surname and capitalized it. This was the pattern I was seeing with the non-null values.\n",
    "\n",
    "# Define a user defined function (UDF) to generate the missing codes\n",
    "def generate_code(surname):\n",
    "    return surname[:3].upper()\n",
    "\n",
    "generate_code_udf = udf(generate_code, StringType())\n",
    "\n",
    "# Apply the (UDF) to fill in the missing codes\n",
    "drivers_df_filled = drivers_df.withColumn(\n",
    "    \"code\",\n",
    "    when(col(\"code\") == \"\\\\N\", generate_code_udf(col(\"surname\"))).otherwise(col(\"code\"))\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "drivers_df_filled.select(\"driverId\", \"forename\", \"surname\", \"code\").show(10)\n",
    "\n",
    "# Outputting filled-in driver codes to AWS\n",
    "drivers_df_filled.coalesce(1).write.mode('overwrite').option(\"header\", True)\\\n",
    "    .csv(\"s3://jwj2123-gr5069/processed/Assignment #2/missing_code/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe3362f-7aab-4b10-a638-e114ea49d770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###4.[20 pts] Who is the youngest and oldest driver for each race? Create a new column called “Age”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79599d41-097b-45b1-84d9-134c2482e8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To find the age of each driver, I joined the 'pit_stops' dataset with the 'drivers' dataset and the 'races' dataset. I then calculated the age of each driver by subtracting the date of birth from the race date.\n",
    "\n",
    "# Convert date columns to date type if needed\n",
    "drivers_df = drivers_df.withColumn(\"dob\", F.to_date(\"dob\"))\n",
    "races_df = races_df.withColumn(\"date\", F.to_date(\"date\"))\n",
    "\n",
    "# Join the dataframes\n",
    "joined_df = results_df.join(drivers_df, \"driverId\").join(races_df, \"raceId\")\n",
    "\n",
    "# Calculate age: difference in years between race date and date of birth\n",
    "# This subtracts birth year from race year, then adjusts if birthday hasn't occurred yet that year\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"Age\",\n",
    "    F.when(\n",
    "        (F.month(joined_df.date) > F.month(joined_df.dob)) | \n",
    "        ((F.month(joined_df.date) == F.month(joined_df.dob)) & \n",
    "         (F.dayofmonth(joined_df.date) >= F.dayofmonth(joined_df.dob))),\n",
    "        F.year(joined_df.date) - F.year(joined_df.dob)\n",
    "    ).otherwise(\n",
    "        F.year(joined_df.date) - F.year(joined_df.dob) - 1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Find youngest and oldest driver for each race\n",
    "window_spec = Window.partitionBy(\"raceId\")\n",
    "\n",
    "result_df = joined_df.withColumn(\n",
    "    \"min_age\", F.min(\"Age\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"max_age\", F.max(\"Age\").over(window_spec)\n",
    ").filter(\n",
    "    (F.col(\"Age\") == F.col(\"min_age\")) | (F.col(\"Age\") == F.col(\"max_age\"))\n",
    ").select(\n",
    "    \"raceId\", \n",
    "    \"name\", \n",
    "    \"date\", \n",
    "    \"driverId\", \n",
    "    \"forename\", \n",
    "    \"surname\", \n",
    "    \"Age\",\n",
    "    F.when(F.col(\"Age\") == F.col(\"min_age\"), \"Youngest\")\n",
    "     .otherwise(\"Oldest\").alias(\"Age_Category\")\n",
    ").orderBy(\"raceId\", \"Age_Category\")\n",
    "\n",
    "# Show results\n",
    "result_df.show()\n",
    "\n",
    "# Outputting youngest and oldest drivers per race to AWS as CSV\n",
    "result_df.coalesce(1).write.mode('overwrite').option(\"header\", True)\\\n",
    "    .csv(\"s3://jwj2123-gr5069/processed/Assignment #2/age/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cafcde8-0fe4-48b4-8497-1799df7555c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5.[20 pts] For a given race, which driver has the most wins and losses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13506f3-041d-4fbb-b0c2-61200d1a8c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify all statusIds that represent a \"Did Not Finish\" (DNF)\n",
    "# Usually any description that is NOT \"Finished\" is a DNF\n",
    "dnf_status_ids = status_df.filter(col(\"status\") != \"Finished\").select(\"statusId\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Filter to races before the given race. Here to get a given race change the 'target_race_id' to the race you want to see. Right now it is set to 841. \n",
    "target_race_id = 841\n",
    "previous_races_df = results_df.filter(col(\"raceId\") < target_race_id)\n",
    "\n",
    "# Create labeled columns\n",
    "labeled_df = previous_races_df.withColumn(\n",
    "    \"win\", when(col(\"positionOrder\") == 1, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"not_completed\", when(col(\"statusId\").isin(dnf_status_ids), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"completed_not_won\", when(\n",
    "        (col(\"positionOrder\") > 1) & (~col(\"statusId\").isin(dnf_status_ids)), 1\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"total_participated\", lit(1)\n",
    ")\n",
    "\n",
    "# Aggregate\n",
    "summary_df = labeled_df.groupBy(\"driverId\").agg(\n",
    "    count(when(col(\"win\") == 1, True)).alias(\"wins\"),\n",
    "    count(when(col(\"completed_not_won\") == 1, True)).alias(\"completed_not_won\"),\n",
    "    count(when(col(\"not_completed\") == 1, True)).alias(\"not_completed\"),\n",
    "    count(col(\"total_participated\")).alias(\"total_races\")\n",
    ")\n",
    "\n",
    "# Join with driver names\n",
    "final_df = summary_df.join(drivers_df.select(\"driverId\", \"surname\"), on=\"driverId\", how=\"left\")\n",
    "\n",
    "# Display results\n",
    "final_df.select(\"surname\", \"wins\", \"completed_not_won\", \"not_completed\", \"total_races\") \\\n",
    "    .orderBy(\"wins\", ascending=False).show()\n",
    "\n",
    "# Outputting to AWS\n",
    "final_df.coalesce(1).write.mode('overwrite').option(\"header\", True)\\\n",
    "    .csv(\"s3://jwj2123-gr5069/processed/Assignment #2/win_losses/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecfa3384-1a10-4166-959f-8a6dc4d46290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###6.[10 pts] Continue exploring the data by answering your own question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585348ef-007a-4edb-ba22-801fd829a9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A queston I will be answeing will be \"Which driver improves the most positions, on average, from their qualifying position to thier race finishing position?\"\n",
    "\n",
    "# Join on raceId and driverId to get both qualifying and result info\n",
    "joined_df = qualifying_df.join(\n",
    "    results_df.select(\"raceId\", \"driverId\", \"positionOrder\"),\n",
    "    on=[\"raceId\", \"driverId\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Calculate position change (positive = improved positions)\n",
    "position_diff_df = joined_df.withColumn(\n",
    "    \"position_gain\", col(\"position\") - col(\"positionOrder\")\n",
    ")\n",
    "\n",
    "# Group by driver and calculate average gain, rounding to the nearest whole number\n",
    "avg_gain_df = position_diff_df.groupBy(\"driverId\").agg(\n",
    "    round(avg(\"position_gain\")).alias(\"avg_position_gain\")\n",
    ")\n",
    "\n",
    "# Join with driver names\n",
    "final_gain_df = avg_gain_df.join(\n",
    "    drivers_df.select(\"driverId\", \"surname\"),\n",
    "    on=\"driverId\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Order by most average positions gained\n",
    "display(\n",
    "    final_gain_df.select(\"surname\", \"avg_position_gain\")\n",
    "    .orderBy(\"avg_position_gain\", ascending=False)\n",
    ")\n",
    "\n",
    "# Outputting to AWS\n",
    "final_gain_df.coalesce(1).write.mode('overwrite').option(\"header\", True)\\\n",
    "    .csv(\"s3://jwj2123-gr5069/processed/Assignment #2/driver_improvement/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Applied Data Homework 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
